
import os
import time
import json
import re
from pathlib import Path
from urllib.parse import urljoin, urlparse
import requests
import pandas as pd

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import (
    NoSuchElementException,
    TimeoutException,
    StaleElementReferenceException,
    ElementClickInterceptedException
)
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options

# 1. configuración
DESKTOP = Path.home() / "Desktop"
DOWNLOAD_ROOT = DESKTOP / "SECOP_PAA_Downloads"

# 2. validación URL
BASE_URL = "https://community.secop.gov.co/Public/App/AnnualPurchasingPlanManagementPublic/Index?Country=CO&Page=login&SkinName=CCE&currentLanguage=es-CO"

# 2.1 validar tiempo de espera
SHORT_WAIT = 1.0
MEDIUM_WAIT = 2.0
LONG_WAIT = 5.0

# 2.2 validar extensiones
EXCEL_EXTS = [".xls", ".xlsx", ".xlsm", ".csv"]

def slugify_for_fs(s: str, maxlen=120):
    s = re.sub(r'[\\/*?:"<>|]', "_", s)
    s = re.sub(r'\s+', " ", s).strip()
    return s[:maxlen]

def ensure_dir(path: Path):
    path.mkdir(parents=True, exist_ok=True)

def download_file(url, dest_path):
    # bajar (header user-agent simple)
    try:
        resp = requests.get(url, stream=True, timeout=60, headers={"User-Agent":"Mozilla/5.0"})
        resp.raise_for_status()
        with open(dest_path, "wb") as f:
            for chunk in resp.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        return True
    except Exception as e:
        print(f"Error descargando {url}: {e}")
        return False

# 3. scraping

def create_driver(download_dir=None, headless=False):
    chrome_options = Options()
    if not headless:
        chrome_options.add_argument("--start-maximized")
    else:
        chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--window-size=1920,1080")

    prefs = {}
    if download_dir:
        prefs = {
            "download.default_directory": str(download_dir),
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "safebrowsing.enabled": True
        }
        chrome_options.add_experimental_option("prefs", prefs)

    # opciones recomendadas por IA para evitar detección ligera
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def main(year=None, max_pages=None, headless=False):
    # Aseguramos carpeta raíz
    ensure_dir(DOWNLOAD_ROOT)
    driver = create_driver(download_dir=DOWNLOAD_ROOT, headless=headless)
    driver.get(BASE_URL)
    time.sleep(LONG_WAIT)

    # filtrar por año
    if year:
        try:
            year_select = driver.find_element(By.CSS_SELECTOR, "select#Year")
            for opt in year_select.find_elements(By.TAG_NAME, "option"):
                if opt.text.strip() == str(year):
                    opt.click()
                    time.sleep(MEDIUM_WAIT)
                    break
        except Exception:
            print("No se pudo filtrar por año (posible cambio en DOM). Continuamos sin filtrar.")

    # paginación y filas
    page = 1
    while True:
        print(f"Procesando página {page} ...")
        time.sleep(SHORT_WAIT)
        # localiza la tabla de resultados; ajustar selector si cambia
        try:
            rows = driver.find_elements(By.CSS_SELECTOR, "table tbody tr")
        except Exception as e:
            print("No se encontraron filas en la tabla:", e)
            rows = []

        if not rows:
            print("No hay registros en esta página.")
        for r_idx, row in enumerate(rows, start=1):
            try:
                # extraer columnas visibles: Nombre, Fecha public., Valor, Version, Modificado, Estado
                cols = row.find_elements(By.TAG_NAME, "td")
                if not cols:
                    continue

                # la primera celda contiene imagen+nombre; buscamos el nombre visible
                text_cells = [c.text.strip() for c in cols if c.text.strip()]
                
                try:
                    detail_btn = row.find_element(By.XPATH, ".//a[contains(translate(., 'D', 'd'), 'detail') or contains(@title,'Detail') or contains(@title,'Detalle') or contains(., 'Detail')]")
                except NoSuchElementException:
                    try:
                        detail_btn = row.find_element(By.XPATH, ".//a[contains(text(),'Detail') or contains(text(),'Detalle')]")
                    except Exception:
                        detail_btn = None
                if detail_btn:
                    # abrir detalle en nueva pestaña para no perder la lista
                    href = detail_btn.get_attribute("href")
                    if not href:
                        # ejecutar click
                        try:
                            driver.execute_script("arguments[0].scrollIntoView();", detail_btn)
                            detail_btn.click()
                            time.sleep(MEDIUM_WAIT)
                        except Exception as e:
                            print("No se pudo clickear detail:", e)
                            continue
                    else:
                        driver.execute_script("window.open(arguments[0], '_blank');", href)
                        driver.switch_to.window(driver.window_handles[-1])
                        time.sleep(MEDIUM_WAIT)
                else:
                    print("No se encontró botón Detail en fila; saltando.")
                    continue

                # detalle entidad
                html = driver.page_source
                entity_data = {}
                try:
                    # Nombre de la entidad (h1/h2)
                    try:
                        h = driver.find_element(By.CSS_SELECTOR, "h1, h2")
                        entity_data["nombre"] = h.text.strip()
                    except Exception:
                        entity_data["nombre"] = text_cells[0] if text_cells else "Sin nombre"

                    # párrafos/definition list con NIT, Año, Valor, Fecha
                    # buscar elementos con label "NIT" o "Nit" o "NIT:"
                    labels = driver.find_elements(By.XPATH, "//*[contains(.,'NIT') or contains(.,'Nit') or contains(.,'NIT:') or contains(.,'Nit:')]")
                    if labels:
                        text_join = " | ".join([l.text for l in labels])
                        entity_data["raw_labels"] = text_join[:2000]
                except Exception as e:
                    print("No se pudieron extraer todos los metadatos:", e)

                # preparar carpeta por entidad
                nit = ""
                if "nombre" in entity_data:
                    folder_name = slugify_for_fs(entity_data["nombre"])
                else:
                    folder_name = f"entidad_{int(time.time())}"
                # añade el año 
                if year:
                    folder_name = f"{folder_name} ({year})"
                entity_folder = DOWNLOAD_ROOT / folder_name
                ensure_dir(entity_folder)

                # guardar metadata básica
                meta_path = entity_folder / "metadata.json"
                with open(meta_path, "w", encoding="utf-8") as f:
                    json.dump(entity_data, f, ensure_ascii=False, indent=2)
                
                # sugerido por IA

                # Buscar enlaces a archivos/adjuntos en la sección "Attachments" o links que contengan .xls/.xlsx/.csv
                anchors = driver.find_elements(By.XPATH, "//a[contains(@href,'.xls') or contains(@href,'.xlsx') or contains(@href,'.csv') or contains(translate(@href,'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'),'.xls') or contains(translate(@href,'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'),'.xlsx')]")
                # también buscar links con texto 'Download' o 'Descargar'
                anchors_text = driver.find_elements(By.XPATH, "//a[contains(., 'Download') or contains(., 'Descargar') or contains(., 'download') or contains(., 'descargar')]")
                anchors_all = list({a.get_attribute('href'): a for a in anchors + anchors_text if a.get_attribute('href')})
                # descargar todos los que parezcan excel
                for href, a in anchors_all.items():
                    if not href:
                        continue
                    lower = href.lower()
                    ext = Path(urlparse(href).path).suffix
                    if ext in EXCEL_EXTS or any(ext2 in lower for ext2 in EXCEL_EXTS):
                        filename = Path(urlparse(href).path).name or f"archivo_{int(time.time())}{ext}"
                        safe_name = slugify_for_fs(filename)
                        dest = entity_folder / safe_name
                        if dest.exists():
                            print(f"Ya existe {dest.name}, saltando descarga.")
                        else:
                            print(f"Descargando {href} -> {dest}")
                            ok = download_file(href, dest)
                            if not ok:
                                # intento con link absoluto si es relativo
                                if href.startswith("/"):
                                    base = "{uri.scheme}://{uri.netloc}".format(uri=urlparse(BASE_URL))
                                    full = base + href
                                    download_file(full, dest)
                    else:
                        # si el link no termina en excel, pero podría abrir un modal o descarga dinámica
                        # intentar obtener href desde atributo data-url o similar, o click y buscar enlaces generados
                        pass

                # opcional: guardar una vista simple en CSV con links encontrados
                links = [a for a in anchors_all.keys()]
                df = pd.DataFrame({"links": links})
                df.to_csv(entity_folder / "links.csv", index=False)

                if len(driver.window_handles) > 1:
                    driver.close()
                    driver.switch_to.window(driver.window_handles[0])
                    time.sleep(SHORT_WAIT)
                else:
                    try:
                        driver.back()
                        time.sleep(MEDIUM_WAIT)
                    except Exception:
                        pass

            except Exception as e:
                print("Error procesando fila:", e)
                # intentar volver a la lista por seguridad
                try:
                    if len(driver.window_handles) > 1:
                        driver.switch_to.window(driver.window_handles[0])
                except Exception:
                    pass
                continue

        # paginación: intentar click en siguiente
        page += 1
        if max_pages and page > max_pages:
            print("Límite de páginas alcanzado (max_pages).")
            break

        try:
            # selector típico para botón siguiente (ajustar si cambia)
            next_btn = driver.find_element(By.XPATH, "//a[contains(@class,'next') or contains(text(),'Next') or contains(text(),'Siguiente') or contains(@title,'Next')]")
            # si está deshabilitado rompemos
            cls = next_btn.get_attribute("class") or ""
            if "disabled" in cls.lower():
                print("Botón siguiente deshabilitado. Fin.")
                break
            driver.execute_script("arguments[0].scrollIntoView();", next_btn)
            next_btn.click()
            time.sleep(MEDIUM_WAIT)
        except Exception:
            print("No se encontró botón 'siguiente' o paginación distinta. Intento de ruptura.")
            break

    print("Proceso terminado.")
    driver.quit()

if __name__ == "__main__":
    # configuración final: año, max_pages, headless = False para ver navegador
    main(year=2025, max_pages=None, headless=False)

